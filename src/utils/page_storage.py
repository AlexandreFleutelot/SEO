# -*- coding: utf-8 -*-
"""
page_storage.py

Utilitaires pour la sauvegarde et la gestion des pages HTML t√©l√©charg√©es.
"""

import os
import json
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urlparse
import hashlib
import re


def sanitize_filename(url: str) -> str:
    """
    Convertit une URL en nom de fichier s√©curis√©.
    
    Args:
        url (str): URL √† convertir
        
    Returns:
        str: Nom de fichier s√©curis√©
    """
    # Parse l'URL
    parsed = urlparse(url)
    
    # Cr√©er un nom bas√© sur le domaine et le chemin
    domain = parsed.netloc.replace('www.', '')
    path = parsed.path.strip('/')
    
    # Remplacer les caract√®res probl√©matiques
    safe_chars = re.sub(r'[^\w\-_.]', '_', f"{domain}_{path}")
    
    # Limiter la longueur et nettoyer
    safe_chars = safe_chars.replace('__', '_').strip('_')[:100]
    
    return safe_chars


def save_page_content(url: str, html_content: str, response_headers: dict = None) -> str:
    """
    Sauvegarde le contenu HTML d'une page avec ses m√©tadonn√©es.
    
    Args:
        url (str): URL de la page
        html_content (str): Contenu HTML brut
        response_headers (dict, optional): Headers de la r√©ponse HTTP
        
    Returns:
        str: Chemin du fichier sauvegard√©
    """
    # Cr√©er le dossier s'il n'existe pas
    pages_dir = Path("data/pages")
    pages_dir.mkdir(parents=True, exist_ok=True)
    
    # G√©n√©rer le nom de fichier
    safe_filename = sanitize_filename(url)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Nom de fichier avec horodatage
    html_filename = f"{safe_filename}_{timestamp}.html"
    metadata_filename = f"{safe_filename}_{timestamp}_metadata.json"
    
    html_path = pages_dir / html_filename
    metadata_path = pages_dir / metadata_filename
    
    # Sauvegarder le HTML
    try:
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            "url": url,
            "download_date": datetime.now().isoformat(),
            "html_file": html_filename,
            "html_size_bytes": len(html_content.encode('utf-8')),
            "html_size_kb": round(len(html_content.encode('utf-8')) / 1024, 2),
            "content_hash": hashlib.md5(html_content.encode('utf-8')).hexdigest(),
            "response_headers": response_headers or {},
            "domain": urlparse(url).netloc,
            "path": urlparse(url).path
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Page sauvegard√©e: {html_path}")
        print(f"üìã M√©tadonn√©es: {metadata_path}")
        
        return str(html_path)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la sauvegarde de la page: {e}")
        return ""


def get_saved_pages() -> list:
    """
    R√©cup√®re la liste des pages sauvegard√©es avec leurs m√©tadonn√©es.
    
    Returns:
        list: Liste des pages sauvegard√©es avec leurs m√©tadonn√©es
    """
    pages_dir = Path("data/pages")
    
    if not pages_dir.exists():
        return []
    
    saved_pages = []
    
    # Parcourir tous les fichiers metadata
    for metadata_file in pages_dir.glob("*_metadata.json"):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # V√©rifier que le fichier HTML existe encore
            html_path = pages_dir / metadata['html_file']
            if html_path.exists():
                metadata['html_path'] = str(html_path)
                metadata['metadata_path'] = str(metadata_file)
                saved_pages.append(metadata)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la lecture de {metadata_file}: {e}")
    
    # Trier par date de t√©l√©chargement (plus r√©cent en premier)
    saved_pages.sort(key=lambda x: x['download_date'], reverse=True)
    
    return saved_pages


def cleanup_old_pages(max_pages: int = 100, max_days: int = 30):
    """
    Nettoie les anciennes pages sauvegard√©es.
    
    Args:
        max_pages (int): Nombre maximum de pages √† conserver
        max_days (int): √Çge maximum des pages en jours
    """
    pages_dir = Path("data/pages")
    
    if not pages_dir.exists():
        return
    
    saved_pages = get_saved_pages()
    
    # Nettoyer par nombre
    if len(saved_pages) > max_pages:
        pages_to_delete = saved_pages[max_pages:]
        for page in pages_to_delete:
            try:
                os.remove(page['html_path'])
                os.remove(page['metadata_path'])
                print(f"üóëÔ∏è Supprim√©: {page['html_file']} (limite de {max_pages} pages)")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la suppression: {e}")
    
    # Nettoyer par √¢ge
    cutoff_date = datetime.now() - timedelta(days=max_days)
    
    for page in saved_pages:
        try:
            download_date = datetime.fromisoformat(page['download_date'].replace('Z', '+00:00'))
            if download_date < cutoff_date:
                os.remove(page['html_path'])
                os.remove(page['metadata_path'])
                print(f"üóëÔ∏è Supprim√©: {page['html_file']} (plus de {max_days} jours)")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du nettoyage par √¢ge: {e}")


def find_page_by_url(url: str, tolerance_hours: int = 24) -> dict:
    """
    Trouve une page sauvegard√©e correspondant √† l'URL donn√©e.
    
    Args:
        url (str): URL √† chercher
        tolerance_hours (int): Tol√©rance en heures pour consid√©rer une page comme r√©cente
        
    Returns:
        dict: M√©tadonn√©es de la page trouv√©e ou None
    """
    saved_pages = get_saved_pages()
    
    for page in saved_pages:
        if page['url'] == url:
            # V√©rifier si la page est assez r√©cente
            download_date = datetime.fromisoformat(page['download_date'].replace('Z', '+00:00'))
            age_hours = (datetime.now() - download_date).total_seconds() / 3600
            
            if age_hours <= tolerance_hours:
                return page
    
    return None


def get_storage_stats() -> dict:
    """
    Retourne des statistiques sur le stockage des pages.
    
    Returns:
        dict: Statistiques de stockage
    """
    pages_dir = Path("data/pages")
    
    if not pages_dir.exists():
        return {
            "total_pages": 0,
            "total_size_mb": 0,
            "oldest_date": None,
            "newest_date": None
        }
    
    saved_pages = get_saved_pages()
    
    if not saved_pages:
        return {
            "total_pages": 0,
            "total_size_mb": 0,
            "oldest_date": None,
            "newest_date": None
        }
    
    total_size = sum(page.get('html_size_bytes', 0) for page in saved_pages)
    
    dates = [datetime.fromisoformat(page['download_date'].replace('Z', '+00:00')) for page in saved_pages]
    
    return {
        "total_pages": len(saved_pages),
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "oldest_date": min(dates).strftime('%d/%m/%Y %H:%M'),
        "newest_date": max(dates).strftime('%d/%m/%Y %H:%M'),
        "average_size_kb": round(total_size / len(saved_pages) / 1024, 2)
    }