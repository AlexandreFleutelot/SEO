# -*- coding: utf-8 -*-
"""
Analyseur SEO Principal
Point d'entrÃ©e principal pour analyser une page web
"""

import os
import json
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from .config import (
    USER_MESSAGES, DEFAULT_USER_AGENT, REQUEST_TIMEOUT,
    SEO_ANALYSIS_DIR, SEO_SCORES_DIR, get_analysis_config
)
from .modules import (
    analyser_contenu_complet,
    analyser_structure_complete,
    analyser_performance_complete,
    calculer_score_global,
    generer_recommandations
)
from .utils.page_storage import save_page_content


def analyser_page_complete(url: str, options: dict = None) -> dict:
    """
    Analyse complÃ¨te d'une page web avec tous les modules SEO
    
    Args:
        url: URL de la page Ã  analyser
        options: Options d'analyse (optionnel)
        
    Returns:
        dict: RÃ©sultats complets de l'analyse SEO
    """
    print(f"\nğŸš€ DÃ‰BUT DE L'ANALYSE SEO")
    print(f"ğŸŒ URL: {url}")
    print("=" * 80)
    
    # Configuration par dÃ©faut
    if options is None:
        options = get_analysis_config()
    
    # Initialiser les rÃ©sultats
    resultats = {
        'url': url,
        'date_analyse': datetime.now().isoformat(),
        'domaine': urlparse(url).netloc,
        'configuration': options,
        'succes': False,
        'erreurs': [],
        'analyses': {},
        'scores': {},
        'recommandations': {}
    }
    
    try:
        # === Ã‰TAPE 1: RÃ‰CUPÃ‰RATION DE LA PAGE ===
        print("ğŸ“¥ RÃ©cupÃ©ration de la page web...")
        soup, contenu_brut = recuperer_page_web(url)
        
        if not soup:
            resultats['erreurs'].append("Impossible de rÃ©cupÃ©rer le contenu de la page")
            return resultats
        
        # Sauvegarder la page pour cache/debug
        try:
            save_page_content(url, contenu_brut)
        except Exception as e:
            print(f"âš ï¸ Sauvegarde page Ã©chouÃ©e: {e}")
        
        print("âœ… Page rÃ©cupÃ©rÃ©e avec succÃ¨s")
        
        # === Ã‰TAPE 2: ANALYSES PAR CATÃ‰GORIE ===
        print("\nğŸ“Š ANALYSES PAR CATÃ‰GORIE")
        print("-" * 50)
        
        # Analyse du contenu
        try:
            resultats['analyses']['contenu'] = analyser_contenu_complet(soup, url)
        except Exception as e:
            print(f"âŒ Erreur analyse contenu: {e}")
            resultats['erreurs'].append(f"Analyse contenu Ã©chouÃ©e: {e}")
        
        # Analyse de la structure
        try:
            resultats['analyses']['structure'] = analyser_structure_complete(soup, url)
        except Exception as e:
            print(f"âŒ Erreur analyse structure: {e}")
            resultats['erreurs'].append(f"Analyse structure Ã©chouÃ©e: {e}")
        
        # Analyse des performances (si activÃ©e)
        if options.get('performance_enabled', False):
            try:
                resultats['analyses']['performance'] = analyser_performance_complete(url)
            except Exception as e:
                print(f"âŒ Erreur analyse performance: {e}")
                resultats['erreurs'].append(f"Analyse performance Ã©chouÃ©e: {e}")
        else:
            print("âš ï¸ Analyse performance dÃ©sactivÃ©e (pas de clÃ© API)")
        
        # === Ã‰TAPE 3: CALCUL DES SCORES ===
        print("\nğŸ§® CALCUL DES SCORES")
        print("-" * 30)
        
        try:
            resultats['scores'] = calculer_score_global(resultats['analyses'])
            print(f"ğŸ“ˆ Score global: {resultats['scores']['score_global']}/100")
            print(f"ğŸ¯ Niveau: {resultats['scores']['niveau_performance']}")
        except Exception as e:
            print(f"âŒ Erreur calcul scores: {e}")
            resultats['erreurs'].append(f"Calcul scores Ã©chouÃ©: {e}")
        
        # === Ã‰TAPE 4: GÃ‰NÃ‰RATION DES RECOMMANDATIONS ===
        print("\nğŸ’¡ GÃ‰NÃ‰RATION DES RECOMMANDATIONS")
        print("-" * 40)
        
        try:
            resultats['recommandations'] = generer_recommandations(
                resultats['analyses'], 
                resultats['scores']
            )
            nb_reco = sum(len(reco) for reco in resultats['recommandations'].values())
            print(f"âœ… {nb_reco} recommandations gÃ©nÃ©rÃ©es")
        except Exception as e:
            print(f"âŒ Erreur gÃ©nÃ©ration recommandations: {e}")
            resultats['erreurs'].append(f"Recommandations Ã©chouÃ©es: {e}")
        
        # === Ã‰TAPE 5: SAUVEGARDE ===
        print("\nğŸ’¾ SAUVEGARDE DES RÃ‰SULTATS")
        print("-" * 35)
        
        try:
            sauvegarder_resultats(resultats)
            resultats['succes'] = True
            print("âœ… RÃ©sultats sauvegardÃ©s avec succÃ¨s")
        except Exception as e:
            print(f"âŒ Erreur sauvegarde: {e}")
            resultats['erreurs'].append(f"Sauvegarde Ã©chouÃ©e: {e}")
        
        # === RÃ‰SUMÃ‰ FINAL ===
        print("\n" + "=" * 80)
        print("ğŸ‰ ANALYSE TERMINÃ‰E")
        
        if resultats['succes']:
            score = resultats['scores'].get('score_global', 0)
            niveau = resultats['scores'].get('niveau_performance', 'Inconnu')
            print(f"ğŸ“Š Score final: {score}/100 ({niveau})")
            
            forces = resultats['scores'].get('forces', [])
            faiblesses = resultats['scores'].get('faiblesses', [])
            
            if forces:
                print(f"ğŸ’ª Forces: {', '.join(forces)}")
            if faiblesses:
                print(f"âš ï¸ Ã€ amÃ©liorer: {', '.join(faiblesses)}")
        
        if resultats['erreurs']:
            print(f"âš ï¸ {len(resultats['erreurs'])} erreur(s) dÃ©tectÃ©e(s)")
        
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ ERREUR CRITIQUE: {e}")
        resultats['erreurs'].append(f"Erreur critique: {e}")
    
    return resultats


def recuperer_page_web(url: str) -> tuple:
    """
    RÃ©cupÃ¨re le contenu HTML d'une page web
    
    Args:
        url: URL Ã  rÃ©cupÃ©rer
        
    Returns:
        tuple: (objet BeautifulSoup, contenu HTML brut) ou (None, None) si erreur
    """
    headers = {
        'User-Agent': DEFAULT_USER_AGENT,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Cache-Control': 'no-cache'
    }
    
    try:
        print(f"  ğŸ”— Connexion Ã  {url}...")
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        
        if response.status_code != 200:
            print(f"  âŒ Code de statut HTTP: {response.status_code}")
            return None, None
        
        # VÃ©rifier le type de contenu
        content_type = response.headers.get('Content-Type', '')
        if 'text/html' not in content_type:
            print(f"  âš ï¸ Type de contenu non HTML: {content_type}")
        
        # Parser avec BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"  âœ… Page rÃ©cupÃ©rÃ©e ({len(response.text)} caractÃ¨res)")
        return soup, response.text
        
    except requests.exceptions.Timeout:
        print(f"  âŒ Timeout aprÃ¨s {REQUEST_TIMEOUT}s")
        return None, None
    except requests.exceptions.ConnectionError:
        print(f"  âŒ Erreur de connexion")
        return None, None
    except requests.exceptions.RequestException as e:
        print(f"  âŒ Erreur de requÃªte: {e}")
        return None, None
    except Exception as e:
        print(f"  âŒ Erreur inattendue: {e}")
        return None, None


def sauvegarder_resultats(resultats: dict) -> None:
    """
    Sauvegarde les rÃ©sultats dans les fichiers JSON
    
    Args:
        resultats: Dictionnaire des rÃ©sultats complets
    """
    url = resultats['url']
    domaine = resultats['domaine']
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Nom de fichier sÃ©curisÃ©
    nom_fichier = nettoyer_nom_fichier(url)
    
    # Sauvegarder le rapport brut complet
    fichier_brut = SEO_ANALYSIS_DIR / f"rapport_{nom_fichier}.json"
    print(f"  ğŸ’¾ Sauvegarde rapport brut: {fichier_brut.name}")
    
    with open(fichier_brut, 'w', encoding='utf-8') as f:
        json.dump(resultats, f, indent=2, ensure_ascii=False)
    
    # Sauvegarder le rapport de scores simplifiÃ©
    if 'scores' in resultats and resultats['scores']:
        rapport_scores = {
            'url': url,
            'domaine': domaine,
            'date_analyse': resultats['date_analyse'],
            'score_global': resultats['scores'].get('score_global', 0),
            'niveau_performance': resultats['scores'].get('niveau_performance', 'Inconnu'),
            'scores_categories': resultats['scores'].get('scores_categories', {}),
            'forces': resultats['scores'].get('forces', []),
            'faiblesses': resultats['scores'].get('faiblesses', []),
            'nombre_recommandations': sum(len(reco) for reco in resultats.get('recommandations', {}).values()),
            'succes': resultats['succes']
        }
        
        fichier_scores = SEO_SCORES_DIR / f"scores_{nom_fichier}.json"
        print(f"  ğŸ“Š Sauvegarde scores: {fichier_scores.name}")
        
        with open(fichier_scores, 'w', encoding='utf-8') as f:
            json.dump(rapport_scores, f, indent=2, ensure_ascii=False)


def nettoyer_nom_fichier(url: str) -> str:
    """
    Convertit une URL en nom de fichier sÃ©curisÃ©
    
    Args:
        url: URL Ã  convertir
        
    Returns:
        str: Nom de fichier nettoyÃ©
    """
    import re
    
    # Supprimer le protocole
    nom = url.replace('https://', '').replace('http://', '')
    
    # Remplacer les caractÃ¨res spÃ©ciaux
    nom = re.sub(r'[^\w\-_.]', '_', nom)
    
    # Supprimer les underscores multiples
    nom = re.sub(r'_+', '_', nom)
    
    # Limiter la longueur
    if len(nom) > 100:
        nom = nom[:100]
    
    # Supprimer les underscores en dÃ©but/fin
    nom = nom.strip('_')
    
    return nom if nom else 'page_inconnue'


def main():
    """Fonction principale pour lancement en ligne de commande"""
    import sys
    
    # RÃ©cupÃ©rer l'URL depuis les variables d'environnement ou arguments
    url = os.getenv('ANALYSIS_URL')
    
    if not url and len(sys.argv) > 1:
        url = sys.argv[1]
    
    if not url:
        print("âŒ Aucune URL spÃ©cifiÃ©e")
        print("ğŸ’¡ Utilisez: ANALYSIS_URL=https://example.com python -m src.analyseur")
        print("ğŸ’¡ Ou: python -m src.analyseur https://example.com")
        return
    
    # Lancer l'analyse
    resultats = analyser_page_complete(url)
    
    # Afficher le rÃ©sumÃ©
    if resultats['succes']:
        print(f"\nğŸ¯ Analyse terminÃ©e avec succÃ¨s pour {url}")
    else:
        print(f"\nâš ï¸ Analyse terminÃ©e avec des erreurs pour {url}")


if __name__ == "__main__":
    main()