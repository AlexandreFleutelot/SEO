# -*- coding: utf-8 -*-
"""
Analyseur SEO Principal
Point d'entr√©e principal pour analyser une page web
"""

import os
import json
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from .config import (
    USER_MESSAGES, DEFAULT_USER_AGENT, REQUEST_TIMEOUT,
    SEO_ANALYSIS_DIR, SEO_SCORES_DIR, get_analysis_config
)
from .modules import (
    analyser_contenu_complet,
    analyser_structure_complete,
    analyser_performance_complete,
    calculer_score_global,
    generer_recommandations
)
from .utils.page_storage import save_page_content


def analyser_page_complete(url: str, options: dict = None) -> dict:
    """
    Analyse compl√®te d'une page web avec tous les modules SEO
    
    Args:
        url: URL de la page √† analyser
        options: Options d'analyse (optionnel)
        
    Returns:
        dict: R√©sultats complets de l'analyse SEO
    """
    print(f"\nüöÄ D√âBUT DE L'ANALYSE SEO")
    print(f"üåê URL: {url}")
    print("=" * 80)
    
    # Configuration par d√©faut
    if options is None:
        options = get_analysis_config()
    
    # Initialiser les r√©sultats
    resultats = {
        'url': url,
        'date_analyse': datetime.now().isoformat(),
        'domaine': urlparse(url).netloc,
        'configuration': options,
        'succes': False,
        'erreurs': [],
        'analyses': {},
        'scores': {},
        'recommandations': {}
    }
    
    try:
        # === √âTAPE 1: R√âCUP√âRATION DE LA PAGE ===
        print("üì• R√©cup√©ration de la page web...")
        soup, contenu_brut = recuperer_page_web(url)
        
        if not soup:
            resultats['erreurs'].append("Impossible de r√©cup√©rer le contenu de la page")
            return resultats
        
        # Sauvegarder la page pour cache/debug
        try:
            save_page_content(url, contenu_brut)
        except Exception as e:
            print(f"‚ö†Ô∏è Sauvegarde page √©chou√©e: {e}")
        
        print("‚úÖ Page r√©cup√©r√©e avec succ√®s")
        
        # === √âTAPE 2: ANALYSES PAR CAT√âGORIE ===
        print("\nüìä ANALYSES PAR CAT√âGORIE")
        print("-" * 50)
        
        # Analyse du contenu
        try:
            resultats['analyses']['contenu'] = analyser_contenu_complet(soup, url)
        except Exception as e:
            print(f"‚ùå Erreur analyse contenu: {e}")
            resultats['erreurs'].append(f"Analyse contenu √©chou√©e: {e}")
        
        # Analyse de la structure
        try:
            resultats['analyses']['structure'] = analyser_structure_complete(soup, url)
        except Exception as e:
            print(f"‚ùå Erreur analyse structure: {e}")
            resultats['erreurs'].append(f"Analyse structure √©chou√©e: {e}")
        
        # Analyse des performances (si activ√©e)
        if options.get('performance_enabled', False):
            try:
                resultats['analyses']['performance'] = analyser_performance_complete(url)
            except Exception as e:
                print(f"‚ùå Erreur analyse performance: {e}")
                resultats['erreurs'].append(f"Analyse performance √©chou√©e: {e}")
        else:
            print("‚ö†Ô∏è Analyse performance d√©sactiv√©e (pas de cl√© API)")
        
        # === √âTAPE 3: CALCUL DES SCORES ===
        print("\nüßÆ CALCUL DES SCORES")
        print("-" * 30)
        
        try:
            resultats['scores'] = calculer_score_global(resultats['analyses'])
            print(f"üìà Score global: {resultats['scores']['score_global']}/100")
            print(f"üéØ Niveau: {resultats['scores']['niveau_performance']}")
        except Exception as e:
            print(f"‚ùå Erreur calcul scores: {e}")
            resultats['erreurs'].append(f"Calcul scores √©chou√©: {e}")
        
        # === √âTAPE 4: G√âN√âRATION DES RECOMMANDATIONS ===
        print("\nüí° G√âN√âRATION DES RECOMMANDATIONS")
        print("-" * 40)
        
        try:
            resultats['recommandations'] = generer_recommandations(
                resultats['analyses'], 
                resultats['scores']
            )
            nb_reco = sum(len(reco) for reco in resultats['recommandations'].values())
            print(f"‚úÖ {nb_reco} recommandations g√©n√©r√©es")
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration recommandations: {e}")
            resultats['erreurs'].append(f"Recommandations √©chou√©es: {e}")
        
        # === √âTAPE 5: SAUVEGARDE ===
        print("\nüíæ SAUVEGARDE DES R√âSULTATS")
        print("-" * 35)
        
        try:
            sauvegarder_resultats(resultats)
            resultats['succes'] = True
            print("‚úÖ R√©sultats sauvegard√©s avec succ√®s")
        except Exception as e:
            print(f"‚ùå Erreur sauvegarde: {e}")
            resultats['erreurs'].append(f"Sauvegarde √©chou√©e: {e}")
        
        # === R√âSUM√â FINAL ===
        print("\n" + "=" * 80)
        print("üéâ ANALYSE TERMIN√âE")
        
        if resultats['succes']:
            score = resultats['scores'].get('score_global', 0)
            niveau = resultats['scores'].get('niveau_performance', 'Inconnu')
            print(f"üìä Score final: {score}/100 ({niveau})")
            
            forces = resultats['scores'].get('forces', [])
            faiblesses = resultats['scores'].get('faiblesses', [])
            
            if forces:
                print(f"üí™ Forces: {', '.join(forces)}")
            if faiblesses:
                print(f"‚ö†Ô∏è √Ä am√©liorer: {', '.join(faiblesses)}")
        
        if resultats['erreurs']:
            print(f"‚ö†Ô∏è {len(resultats['erreurs'])} erreur(s) d√©tect√©e(s)")
        
        print("=" * 80)
        
    except Exception as e:
        print(f"‚ùå ERREUR CRITIQUE: {e}")
        resultats['erreurs'].append(f"Erreur critique: {e}")
    
    return resultats


def recuperer_page_web(url: str) -> tuple:
    """
    R√©cup√®re le contenu HTML d'une page web
    
    Args:
        url: URL √† r√©cup√©rer
        
    Returns:
        tuple: (objet BeautifulSoup, contenu HTML brut) ou (None, None) si erreur
    """
    headers = {
        'User-Agent': DEFAULT_USER_AGENT,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Cache-Control': 'no-cache'
    }
    
    try:
        print(f"  üîó Connexion √† {url}...")
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        
        if response.status_code != 200:
            print(f"  ‚ùå Code de statut HTTP: {response.status_code}")
            return None, None
        
        # V√©rifier le type de contenu
        content_type = response.headers.get('Content-Type', '')
        if 'text/html' not in content_type:
            print(f"  ‚ö†Ô∏è Type de contenu non HTML: {content_type}")
        
        # Parser avec BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"  ‚úÖ Page r√©cup√©r√©e ({len(response.text)} caract√®res)")
        return soup, response.text
        
    except requests.exceptions.Timeout:
        print(f"  ‚ùå Timeout apr√®s {REQUEST_TIMEOUT}s")
        return None, None
    except requests.exceptions.ConnectionError:
        print(f"  ‚ùå Erreur de connexion")
        return None, None
    except requests.exceptions.RequestException as e:
        print(f"  ‚ùå Erreur de requ√™te: {e}")
        return None, None
    except Exception as e:
        print(f"  ‚ùå Erreur inattendue: {e}")
        return None, None


def sauvegarder_resultats(resultats: dict) -> None:
    """
    Sauvegarde les r√©sultats dans les fichiers JSON
    
    Args:
        resultats: Dictionnaire des r√©sultats complets
    """
    url = resultats['url']
    domaine = resultats['domaine']
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Nom de fichier s√©curis√©
    nom_fichier = nettoyer_nom_fichier(url)
    
    # Sauvegarder le rapport brut complet
    fichier_brut = SEO_ANALYSIS_DIR / f"rapport_{nom_fichier}.json"
    print(f"  üíæ Sauvegarde rapport brut: {fichier_brut.name}")
    
    with open(fichier_brut, 'w', encoding='utf-8') as f:
        json.dump(resultats, f, indent=2, ensure_ascii=False)
    
    # Sauvegarder le rapport de scores simplifi√©
    if 'scores' in resultats and resultats['scores']:
        rapport_scores = {
            'url': url,
            'domaine': domaine,
            'date_analyse': resultats['date_analyse'],
            'score_global': resultats['scores'].get('score_global', 0),
            'niveau_performance': resultats['scores'].get('niveau_performance', 'Inconnu'),
            'scores_categories': resultats['scores'].get('scores_categories', {}),
            'forces': resultats['scores'].get('forces', []),
            'faiblesses': resultats['scores'].get('faiblesses', []),
            'nombre_recommandations': sum(len(reco) for reco in resultats.get('recommandations', {}).values()),
            'succes': resultats['succes']
        }
        
        fichier_scores = SEO_SCORES_DIR / f"scores_{nom_fichier}.json"
        print(f"  üìä Sauvegarde scores: {fichier_scores.name}")
        
        with open(fichier_scores, 'w', encoding='utf-8') as f:
            json.dump(rapport_scores, f, indent=2, ensure_ascii=False)


def nettoyer_nom_fichier(url: str) -> str:
    """
    Convertit une URL en nom de fichier s√©curis√©
    
    Args:
        url: URL √† convertir
        
    Returns:
        str: Nom de fichier nettoy√©
    """
    import re
    
    # Supprimer le protocole
    nom = url.replace('https://', '').replace('http://', '')
    
    # Remplacer les caract√®res sp√©ciaux
    nom = re.sub(r'[^\w\-_.]', '_', nom)
    
    # Supprimer les underscores multiples
    nom = re.sub(r'_+', '_', nom)
    
    # Limiter la longueur
    if len(nom) > 100:
        nom = nom[:100]
    
    # Supprimer les underscores en d√©but/fin
    nom = nom.strip('_')
    
    return nom if nom else 'page_inconnue'


def main():
    """Fonction principale pour lancement en ligne de commande"""
    import sys
    
    # R√©cup√©rer l'URL depuis les variables d'environnement ou arguments
    url = os.getenv('ANALYSIS_URL')
    
    if not url and len(sys.argv) > 1:
        url = sys.argv[1]
    
    if not url:
        print("‚ùå Aucune URL sp√©cifi√©e")
        print("üí° Utilisez: ANALYSIS_URL=https://example.com python -m src.analyseur")
        print("üí° Ou: python -m src.analyseur https://example.com")
        return
    
    # Lancer l'analyse
    resultats = analyser_page_complete(url)
    
    # Afficher le r√©sum√©
    if resultats['succes']:
        print(f"\nüéØ Analyse termin√©e avec succ√®s pour {url}")
    else:
        print(f"\n‚ö†Ô∏è Analyse termin√©e avec des erreurs pour {url}")


if __name__ == "__main__":
    main()