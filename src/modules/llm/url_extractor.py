# -*- coding: utf-8 -*-
"""
URL Extractor - Module sp√©cialis√© dans l'extraction d'URLs depuis les r√©ponses LLM
Strat√©gies multiples pour garantir la r√©cup√©ration des sources avec URLs
"""

import re
import requests
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urlparse, urljoin
from .llm_providers import LLMProviderManager


class URLExtractor:
    """Extracteur sp√©cialis√© dans la r√©cup√©ration d'URLs depuis les r√©ponses LLM"""
    
    def __init__(self, llm_manager: LLMProviderManager):
        self.llm_manager = llm_manager
        
        # Patterns pour d√©tecter les URLs
        self.url_patterns = [
            r'https?://[^\s\]\)\,\;\!\?\"\']+',  # URLs compl√®tes
            r'www\.[^\s\]\)\,\;\!\?\"\']+',      # URLs commen√ßant par www
            r'Source:\s*\[([^\]]+)\]\s*-\s*URL:\s*(https?://[^\s]+)',  # Format structur√©
            r'(?:source|r√©f√©rence|lien):\s*(https?://[^\s]+)',         # Format libre
        ]
        
        # Domaines √† exclure (peu fiables pour les sources)
        self.excluded_domains = {
            'google.com', 'bing.com', 'yahoo.com', 'duckduckgo.com',
            'facebook.com', 'twitter.com', 'x.com', 'linkedin.com',
            'instagram.com', 'youtube.com', 'tiktok.com',
            'bit.ly', 'tinyurl.com', 'short.ly', 't.co'
        }
    
    
    def extraire_urls_depuis_reponse(self, provider_name: str, question: str, 
                                   reponse_initiale: str) -> List[Dict[str, Any]]:
        """
        Extrait les URLs depuis une r√©ponse LLM avec strat√©gies multiples et validation SEO
        
        Args:
            provider_name: Nom du provider LLM
            question: Question originale
            reponse_initiale: Premi√®re r√©ponse du LLM
            
        Returns:
            list: Sources avec URLs extraites et valid√©es pour SEO
        """
        print(f"  üîó Extraction URLs avanc√©e ({provider_name})...")
        
        sources = []
        
        # Strat√©gie 1: Parser la r√©ponse initiale
        sources.extend(self._parser_urls_reponse_initiale(reponse_initiale))
        
        # Strat√©gie 2: Si pas assez d'URLs, demande explicite de sources
        if len(sources) < 2:  # Seuil minimum d'URLs
            sources_supplementaires = self._demander_sources_explicites(provider_name, question, reponse_initiale)
            sources.extend(sources_supplementaires)
        
        # Strat√©gie 3: Si encore insuffisant, requ√™te de citation forc√©e
        if len(sources) < 1:
            sources_forcees = self._forcer_citations_sources(provider_name, question)
            sources.extend(sources_forcees)
        
        # Validation avec √©valuation SEO
        sources_validees = self._valider_et_nettoyer_urls(sources)
        
        # Strat√©gie 4: Si trop peu d'URLs exploitables, demander des URLs plus sp√©cifiques
        urls_exploitables = [s for s in sources_validees if s.get('exploitable_seo', False)]
        if len(urls_exploitables) < 2:
            print(f"    üéØ Seulement {len(urls_exploitables)} URLs exploitables - demande d'URLs sp√©cifiques...")
            sources_specifiques = self._demander_urls_specifiques(provider_name, question)
            if sources_specifiques:
                sources_specifiques_validees = self._valider_et_nettoyer_urls(sources_specifiques)
                sources_validees.extend(sources_specifiques_validees)
        
        # D√©duplication finale
        sources_finales = self._deduplication_finale(sources_validees)
        
        print(f"  ‚úÖ {len(sources_finales)} URLs finales extraites et valid√©es")
        return sources_finales
    
    
    def _parser_urls_reponse_initiale(self, reponse: str) -> List[Dict[str, Any]]:
        """Parse la r√©ponse initiale pour extraire les URLs"""
        sources = []
        urls_trouvees = set()  # √âviter les doublons
        
        # Essayer chaque pattern d'extraction
        for pattern in self.url_patterns:
            matches = re.finditer(pattern, reponse, re.IGNORECASE)
            
            for match in matches:
                if 'Source:' in match.group(0) and len(match.groups()) >= 2:
                    # Format structur√© : Source: [Nom] - URL: https://...
                    nom_source = match.group(1).strip()
                    url = match.group(2).strip()
                else:
                    # URL simple trouv√©e
                    url = match.group(0).strip()
                    nom_source = self._extraire_nom_depuis_url(url)
                
                # Nettoyer l'URL
                url = self._nettoyer_url(url)
                
                if url and url not in urls_trouvees and self._url_valide(url):
                    urls_trouvees.add(url)
                    
                    sources.append({
                        'nom': nom_source,
                        'url': url,
                        'domaine': urlparse(url).netloc,
                        'methode_extraction': 'parsing_initial',
                        'contexte': self._extraire_contexte_url(reponse, url)
                    })
        
        return sources
    
    
    def _demander_sources_explicites(self, provider_name: str, question: str, 
                                   reponse_initiale: str) -> List[Dict[str, Any]]:
        """Demande explicitement les sources avec URLs au LLM"""
        print(f"    üîç Demande sources explicites...")
        
        prompt_sources = f"""
Tu viens de r√©pondre √† cette question : "{question}"

Ta r√©ponse pr√©c√©dente :
{reponse_initiale[:500]}...

üö® URGENCE: Je DOIS ABSOLUMENT avoir des sources avec URLs SP√âCIFIQUES pour cette r√©ponse !

üîó MISSION CRITIQUE - SOURCES AVEC PAGES PR√âCISES :
M√™me si tes donn√©es ne sont pas parfaitement r√©centes, tu DOIS me fournir des URLs SP√âCIFIQUES pointant vers des ARTICLES, GUIDES ou DOSSIERS d√©taill√©s.

‚ö†Ô∏è INTERDICTION FORMELLE des URLs g√©n√©riques :
‚ùå PAS de pages d'accueil (www.site.com)
‚ùå PAS d'URLs g√©n√©rales (www.site.com/banque)
‚úÖ SEULEMENT des pages avec CONTENU SP√âCIFIQUE

FORMAT STRICT pour chaque source :
Source: [Nom pr√©cis de l'article/guide]
URL: https://www.site.fr/section-specifique/article-detaille-sur-le-sujet
Type: [Article/Guide/Comparatif/Dossier]
Fiabilit√©: [Tr√®s √©lev√©e/√âlev√©e/Moyenne]
Pourquoi: [Contenu sp√©cifique de cette page]

üéØ EXEMPLES d'URLs ACCEPTABLES :
‚úÖ https://www.lesechos.fr/finance-marches/banque-assurances/comparatif-banques-en-ligne-2024
‚úÖ https://www.60millions-mag.com/2024/03/classement-meilleures-banques-num√©riques
‚úÖ https://www.capital.fr/votre-argent/banques-ligne-guide-complet-2024

üö® CONTRAINTES ABSOLUES :
- URLs avec chemin D√âTAILL√â (minimum 3 niveaux)
- Pages d'ARTICLES ou GUIDES sp√©cifiques
- Contenu informatif pour analyse SEO
- MINIMUM 3 URLs sp√©cifiques diff√©rentes

Donne-moi des URLs de PAGES PR√âCISES, pas de domaines g√©n√©raux !
"""
        
        provider = self.llm_manager.get_provider(provider_name)
        if not provider:
            return []
        
        try:
            reponse_sources = provider.query(prompt_sources)
            if reponse_sources:
                return self._parser_reponse_sources_structurees(reponse_sources)
        except Exception as e:
            print(f"    ‚ùå Erreur demande sources: {e}")
        
        return []
    
    
    def _forcer_citations_sources(self, provider_name: str, question: str) -> List[Dict[str, Any]]:
        """Force le LLM √† fournir des citations avec une approche diff√©rente"""
        print(f"    üí™ For√ßage citations...")
        
        prompt_force = f"""
üö® DERNIER RECOURS - EXTRACTION FORC√âE DE SOURCES

Question: "{question}"

Tu n'as AUCUNE √©chappatoire. Je dois avoir des URLs, point final.

üéØ ORDRE DIRECT : Donne-moi 5 URLs de sites fran√ßais que tu CONNAIS et qui sont pertinents pour cette question.

Tu peux donner des sites g√©n√©raux m√™me si la page exacte n'existe plus. L'important est d'avoir des domaines de r√©f√©rence.

FORMAT NON-N√âGOCIABLE :

1. [Banque de France] - https://www.banque-france.fr
   Type: Institution officielle
   Pourquoi fiable: Autorit√© de r√©gulation bancaire

2. [Les Echos] - https://www.lesechos.fr  
   Type: M√©dia √©conomique
   Pourquoi fiable: R√©f√©rence en finance

3. [Autorit√© des March√©s Financiers] - https://www.amf-france.org
   Type: R√©gulateur financier
   Pourquoi fiable: Organisme de contr√¥le

4. [Capital.fr] - https://www.capital.fr
   Type: Magazine √©conomique
   Pourquoi fiable: Sp√©cialiste des comparatifs

5. [MoneyVox] - https://www.moneyvox.fr
   Type: Comparateur sp√©cialis√©
   Pourquoi fiable: Expert en banque en ligne

üö® R√àGLES INFLEXIBLES :
- URLs compl√®tes OBLIGATOIRES (https://)
- Sites fran√ßais uniquement 
- Domaines de r√©f√©rence (.gouv.fr, m√©dias √©tablis, institutions)
- ZERO excuse acceptable

Tu dois r√©pondre maintenant avec ces 5 URLs !
"""
        
        provider = self.llm_manager.get_provider(provider_name)
        if not provider:
            return []
        
        try:
            reponse_force = provider.query(prompt_force)
            if reponse_force:
                return self._parser_citations_forcees(reponse_force)
        except Exception as e:
            print(f"    ‚ùå Erreur for√ßage citations: {e}")
        
        return []
    
    
    def _parser_reponse_sources_structurees(self, reponse: str) -> List[Dict[str, Any]]:
        """Parse une r√©ponse avec sources structur√©es"""
        sources = []
        
        # Pattern pour le format structur√© demand√©
        pattern = r'Source:\s*([^\n]+)\s*\nURL:\s*(https?://[^\s]+)\s*\n(?:Fiabilit√©:\s*([^\n]+)\s*\n)?(?:Pourquoi:\s*([^\n]+))?'
        
        for match in re.finditer(pattern, reponse, re.MULTILINE | re.IGNORECASE):
            nom = match.group(1).strip()
            url = match.group(2).strip()
            fiabilite = match.group(3).strip() if match.group(3) else "Moyenne"
            explication = match.group(4).strip() if match.group(4) else ""
            
            if self._url_valide(url):
                sources.append({
                    'nom': nom,
                    'url': url,
                    'domaine': urlparse(url).netloc,
                    'fiabilite': fiabilite,
                    'explication': explication,
                    'methode_extraction': 'demande_explicite'
                })
        
        return sources
    
    
    def _parser_citations_forcees(self, reponse: str) -> List[Dict[str, Any]]:
        """Parse les citations obtenues par for√ßage"""
        sources = []
        
        # Pattern pour le format num√©rot√©
        pattern = r'(\d+)\.\s*\[([^\]]+)\]\s*-\s*(https?://[^\s]+)\s*\n\s*Type:\s*([^\n]+)\s*\n\s*Pourquoi fiable:\s*([^\n]+)'
        
        for match in re.finditer(pattern, reponse, re.MULTILINE | re.IGNORECASE):
            numero = match.group(1)
            nom = match.group(2).strip()
            url = match.group(3).strip()
            type_source = match.group(4).strip()
            raison = match.group(5).strip()
            
            if self._url_valide(url):
                sources.append({
                    'nom': nom,
                    'url': url,
                    'domaine': urlparse(url).netloc,
                    'type_source': type_source,
                    'raison_fiabilite': raison,
                    'position': int(numero),
                    'methode_extraction': 'citation_forcee'
                })
        
        return sources
    
    
    def _valider_et_nettoyer_urls(self, sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Valide et nettoie la liste des URLs avec √©valuation SEO"""
        sources_validees = []
        sources_rejetees = []
        urls_vues = set()
        
        for source in sources:
            url = source.get('url', '')
            
            # Nettoyer l'URL
            url_nettoyee = self._nettoyer_url(url)
            
            if (url_nettoyee and 
                url_nettoyee not in urls_vues and 
                self._url_valide(url_nettoyee) and 
                self._domaine_autorise(url_nettoyee)):
                
                # √âvaluer l'exploitabilit√© SEO
                est_exploitable, raison_seo = self._est_url_exploitable_seo(url_nettoyee)
                
                urls_vues.add(url_nettoyee)
                
                # Enrichir avec des informations suppl√©mentaires
                source_enrichie = source.copy()
                source_enrichie['url'] = url_nettoyee
                source_enrichie['domaine'] = urlparse(url_nettoyee).netloc
                source_enrichie['fiabilite_domaine'] = self._evaluer_fiabilite_domaine(url_nettoyee)
                source_enrichie['accessible'] = self._tester_accessibilite_url(url_nettoyee)
                source_enrichie['exploitable_seo'] = est_exploitable
                source_enrichie['raison_seo'] = raison_seo
                
                if est_exploitable:
                    sources_validees.append(source_enrichie)
                    print(f"      ‚úÖ URL exploitable: {url_nettoyee}")
                else:
                    sources_rejetees.append(source_enrichie)
                    print(f"      ‚ö†Ô∏è URL rejet√©e: {url_nettoyee} ({raison_seo})")
        
        # Afficher le r√©sum√©
        if sources_rejetees:
            print(f"    üìä Bilan: {len(sources_validees)} URLs exploitables, {len(sources_rejetees)} rejet√©es")
        
        return sources_validees
    
    
    def _nettoyer_url(self, url: str) -> str:
        """Nettoie une URL"""
        if not url:
            return ""
        
        # Supprimer les espaces et caract√®res ind√©sirables
        url = url.strip().rstrip('.,;!?"\')]}')
        
        # Ajouter https:// si manquant
        if url.startswith('www.'):
            url = 'https://' + url
        
        # Supprimer les param√®tres de tracking communs
        url = re.sub(r'[?&](utm_[^&]+|gclid=[^&]+|fbclid=[^&]+)', '', url)
        
        return url
    
    
    def _url_valide(self, url: str) -> bool:
        """V√©rifie si une URL est valide"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc]) and result.scheme in ['http', 'https']
        except:
            return False
    
    
    def _est_url_exploitable_seo(self, url: str) -> tuple[bool, str]:
        """
        √âvalue si une URL est exploitable pour l'analyse SEO
        
        Returns:
            tuple: (est_exploitable, raison)
        """
        try:
            parsed = urlparse(url)
            path = parsed.path.lower()
            
            # URLs g√©n√©riques peu exploitables
            urls_generiques = [
                '/',           # Page d'accueil
                '',            # Domaine seul
                '/index',      # Page d'index
                '/home',       # Page d'accueil
                '/accueil',    # Page d'accueil fran√ßaise
            ]
            
            if path in urls_generiques:
                return False, "URL g√©n√©rique (page d'accueil)"
            
            # URLs sans contenu sp√©cifique
            if len(path) < 3:
                return False, "URL trop courte, manque de sp√©cificit√©"
            
            # Sections g√©n√©rales peu exploitables
            sections_generales = [
                '/contact',
                '/mentions-legales', 
                '/conditions',
                '/privacy',
                '/about',
                '/a-propos'
            ]
            
            if any(section in path for section in sections_generales):
                return False, "Section g√©n√©rale non-informative"
            
            # URLs avec param√®tres de recherche ou tracking
            if parsed.query:
                # Filtrer les param√®tres de tracking
                query_params = parsed.query.lower()
                if any(param in query_params for param in ['utm_', 'gclid', 'fbclid', 'ref=']):
                    return False, "URL avec param√®tres de tracking"
            
            # Bonnes URLs pour SEO : articles, guides, comparatifs
            indicateurs_bons = [
                'article', 'guide', 'comparatif', 'conseil', 'actualite',
                'dossier', 'analyse', 'test', 'avis', 'selection',
                'meilleur', 'top-', 'classement', '2024', '2025'
            ]
            
            if any(indicateur in path for indicateur in indicateurs_bons):
                return True, "URL sp√©cifique avec contenu informatif"
            
            # URL avec chemin structur√© (au moins 2 niveaux)
            path_parts = [p for p in path.split('/') if p]
            if len(path_parts) >= 2:
                return True, "URL structur√©e avec contenu sp√©cialis√©"
            
            return True, "URL acceptable pour analyse"
            
        except:
            return False, "Erreur parsing URL"
    
    
    def _domaine_autorise(self, url: str) -> bool:
        """V√©rifie si le domaine est autoris√© (pas dans la liste d'exclusion)"""
        try:
            domaine = urlparse(url).netloc.lower()
            # Supprimer les sous-domaines pour v√©rification
            domaine_principal = '.'.join(domaine.split('.')[-2:]) if '.' in domaine else domaine
            return domaine_principal not in self.excluded_domains
        except:
            return False
    
    
    def _evaluer_fiabilite_domaine(self, url: str) -> str:
        """√âvalue la fiabilit√© d'un domaine"""
        try:
            domaine = urlparse(url).netloc.lower()
            
            # Domaines haute fiabilit√©
            if any(ext in domaine for ext in ['.gouv.fr', '.edu', '.org']):
                return "tr√®s √©lev√©e"
            
            # M√©dias fran√ßais reconnus
            medias_fiables = [
                'lemonde.fr', 'lefigaro.fr', 'liberation.fr', 'lepoint.fr',
                'lesechos.fr', 'latribune.fr', 'bfmtv.com', 'franceinfo.fr'
            ]
            if any(media in domaine for media in medias_fiables):
                return "√©lev√©e"
            
            # Institutions/organisations
            if any(terme in domaine for terme in ['banque-france', 'amf-france', 'cnil']):
                return "√©lev√©e"
            
            return "moyenne"
        except:
            return "inconnue"
    
    
    def _tester_accessibilite_url(self, url: str) -> bool:
        """Test rapide d'accessibilit√© de l'URL"""
        try:
            response = requests.head(url, timeout=5, allow_redirects=True)
            return response.status_code < 400
        except:
            return False  # On assume que l'URL n'est pas accessible
    
    
    def _extraire_nom_depuis_url(self, url: str) -> str:
        """Extrait un nom lisible depuis une URL"""
        try:
            parsed = urlparse(url)
            domaine = parsed.netloc
            
            # Supprimer www. si pr√©sent
            if domaine.startswith('www.'):
                domaine = domaine[4:]
            
            # Capitaliser la premi√®re lettre
            return domaine.split('.')[0].capitalize()
        except:
            return "Source inconnue"
    
    
    def _extraire_contexte_url(self, texte: str, url: str, rayon: int = 150) -> str:
        """Extrait le contexte autour d'une URL"""
        index = texte.find(url)
        if index == -1:
            return ""
        
        debut = max(0, index - rayon)
        fin = min(len(texte), index + len(url) + rayon)
        
        return texte[debut:fin].strip()
    
    
    def _demander_urls_specifiques(self, provider_name: str, question: str) -> List[Dict[str, Any]]:
        """Demande sp√©cifiquement des URLs de pages d√©taill√©es exploitables pour SEO"""
        print(f"    üéØ Demande d'URLs sp√©cifiques pour SEO...")
        
        prompt_specifique = f"""
üéØ DEMANDE SP√âCIALIS√âE - URLS DE PAGES D√âTAILL√âES

Question originale: "{question}"

üîç MISSION : Je dois analyser le CONTENU de pages web sp√©cifiques. J'ai besoin d'URLs pointant vers des ARTICLES, GUIDES ou DOSSIERS d√©taill√©s.

‚ö†Ô∏è PROBL√àME CRITIQUE : Tu m'as donn√© des URLs trop g√©n√©riques (pages d'accueil, sections g√©n√©rales).

‚úÖ CE QUE JE VEUX :
- Articles d√©taill√©s avec contenu riche
- Guides pratiques complets
- Comparatifs avec analyses
- Dossiers approfondis
- Pages avec minimum 1000+ mots

‚ùå CE QUE JE NE VEUX PAS :
- Pages d'accueil (www.site.com)
- Sections g√©n√©rales (www.site.com/banque)
- Pages contact/mentions l√©gales

üéØ EXEMPLES D'URLs PARFAITES :
‚úÖ https://www.lesechos.fr/finance-marches/banque-assurances/comparatif-complete-meilleures-banques-en-ligne-2024-details-1234567
‚úÖ https://www.capital.fr/votre-argent/banques-en-ligne-guide-complet-choisir-2024-tarifs-services-avis-12345
‚úÖ https://www.60millions-mag.com/2024/03/test-comparatif-banques-numeriques-boursorama-hello-bank-fortuneo

FORMAT STRICT :
Source: [Titre pr√©cis de l'article/guide]
URL: [URL compl√®te avec chemin d√©taill√©]
Contenu: [Type de contenu - Article/Guide/Comparatif/Test]

MINIMUM 3 URLs de ce type - URLs LONGUES avec CHEMINS D√âTAILL√âS uniquement !
"""
        
        provider = self.llm_manager.get_provider(provider_name)
        if not provider:
            return []
        
        try:
            reponse_specifique = provider.query(prompt_specifique)
            if reponse_specifique:
                return self._parser_urls_specifiques(reponse_specifique)
        except Exception as e:
            print(f"    ‚ùå Erreur demande URLs sp√©cifiques: {e}")
        
        return []
    
    
    def _parser_urls_specifiques(self, reponse: str) -> List[Dict[str, Any]]:
        """Parse les URLs sp√©cifiques demand√©es"""
        sources = []
        
        # Pattern pour le format demand√©
        patterns = [
            r'Source:\s*([^\n]+)\s*\nURL:\s*(https?://[^\s]+)\s*\n(?:Contenu:\s*([^\n]+))?',
            r'‚úÖ\s*(https?://[^\s]+)',  # URLs avec emoji de validation
            r'https?://[^\s]+/[^/\s]+/[^/\s]+/[^\s]+',  # URLs avec chemins longs
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, reponse, re.MULTILINE | re.IGNORECASE)
            for match in matches:
                if len(match.groups()) >= 3:  # Format complet
                    nom = match.group(1).strip()
                    url = match.group(2).strip()
                    contenu = match.group(3).strip() if match.group(3) else "Article"
                    
                    sources.append({
                        'nom': nom,
                        'url': url,
                        'type_contenu': contenu,
                        'methode_extraction': 'demande_specifique'
                    })
                else:  # URL simple
                    url = match.group(0).strip()
                    sources.append({
                        'nom': self._extraire_nom_depuis_url(url),
                        'url': url,
                        'type_contenu': 'Article',
                        'methode_extraction': 'demande_specifique'
                    })
        
        return sources
    
    
    def _deduplication_finale(self, sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """D√©duplication finale des sources avec priorisation des URLs exploitables"""
        sources_uniques = {}
        
        for source in sources:
            url = source.get('url', '')
            if not url:
                continue
            
            # Utiliser l'URL comme cl√© unique
            if url not in sources_uniques:
                sources_uniques[url] = source
            else:
                # Garder la source la plus compl√®te ou la plus exploitable
                source_existante = sources_uniques[url]
                
                # Priorit√© aux sources exploitables pour SEO
                if source.get('exploitable_seo', False) and not source_existante.get('exploitable_seo', False):
                    sources_uniques[url] = source
                elif source.get('exploitable_seo', False) == source_existante.get('exploitable_seo', False):
                    # Si m√™me exploitabilit√©, prendre la plus r√©cente ou compl√®te
                    if len(source.get('nom', '')) > len(source_existante.get('nom', '')):
                        sources_uniques[url] = source
        
        # Trier par exploitabilit√© puis par qualit√©
        sources_finales = list(sources_uniques.values())
        sources_finales.sort(key=lambda x: (
            x.get('exploitable_seo', False),
            x.get('fiabilite_domaine', 'inconnue') == 'tr√®s √©lev√©e',
            x.get('fiabilite_domaine', 'inconnue') == '√©lev√©e',
            len(x.get('nom', ''))
        ), reverse=True)
        
        return sources_finales
    
    
    def generer_rapport_urls(self, sources_par_provider: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """G√©n√®re un rapport consolid√© sur les URLs extraites"""
        
        rapport = {
            'total_sources': 0,
            'sources_par_provider': {},
            'domaines_uniques': set(),
            'fiabilite_distribution': {'tr√®s √©lev√©e': 0, '√©lev√©e': 0, 'moyenne': 0, 'inconnue': 0},
            'methodes_extraction': {},
            'urls_accessibles': 0,
            'urls_inaccessibles': 0
        }
        
        for provider, sources in sources_par_provider.items():
            rapport['sources_par_provider'][provider] = len(sources)
            rapport['total_sources'] += len(sources)
            
            for source in sources:
                # Domaines uniques
                rapport['domaines_uniques'].add(source.get('domaine', ''))
                
                # Distribution de fiabilit√©
                fiabilite = source.get('fiabilite_domaine', 'inconnue')
                if fiabilite in rapport['fiabilite_distribution']:
                    rapport['fiabilite_distribution'][fiabilite] += 1
                
                # M√©thodes d'extraction
                methode = source.get('methode_extraction', 'inconnue')
                rapport['methodes_extraction'][methode] = rapport['methodes_extraction'].get(methode, 0) + 1
                
                # Accessibilit√©
                if source.get('accessible', False):
                    rapport['urls_accessibles'] += 1
                else:
                    rapport['urls_inaccessibles'] += 1
        
        # Convertir set en list pour JSON
        rapport['domaines_uniques'] = list(rapport['domaines_uniques'])
        
        return rapport